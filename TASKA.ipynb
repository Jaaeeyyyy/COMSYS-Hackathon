{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaaeeyyyy/COMSYS-Hackathon/blob/main/TASKA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktLhADAggA2-",
        "outputId": "14b053f9-02e5-4d4c-e9a6-2becfd76842b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Using device: cpu\n",
            "Training samples: 1926\n",
            "Validation samples: 422\n",
            "Classes: ['female', 'male']\n",
            "Class distribution: {0: 303, 1: 1623}\n",
            "Class weights: {0: 3.1782178217821784, 1: 0.5933456561922366}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 83.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original classifier: Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=True)\n",
            "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
            ")\n",
            "Modified classifier: Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=True)\n",
            "  (1): Linear(in_features=1280, out_features=2, bias=True)\n",
            ")\n",
            "ğŸš€ Starting EfficientNet-B0 Training for Gender Classification\n",
            "ğŸ“Š Dataset Info:\n",
            "   Training samples: 1926\n",
            "   Validation samples: 422\n",
            "   Classes: ['female', 'male']\n",
            "   Device: cpu\n",
            "============================================================\n",
            "Starting training...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Batch 0/61, Loss: 0.6624\n",
            "Epoch 1/50, Batch 10/61, Loss: 0.3813\n",
            "Epoch 1/50, Batch 20/61, Loss: 0.2674\n",
            "Epoch 1/50, Batch 30/61, Loss: 0.1510\n",
            "Epoch 1/50, Batch 40/61, Loss: 0.1683\n",
            "Epoch 1/50, Batch 50/61, Loss: 0.1588\n",
            "Epoch 1/50, Batch 60/61, Loss: 0.7105\n",
            "Epoch 1/50\n",
            "Train Loss: 0.2737, Train Acc: 88.53%\n",
            "Val Loss: 0.2156, Val Acc: 91.00%\n",
            "Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "ğŸ’¾ New best model saved! Validation Accuracy: 91.00%\n",
            "Epoch 2/50, Batch 0/61, Loss: 0.1712\n",
            "Epoch 2/50, Batch 10/61, Loss: 0.3508\n",
            "Epoch 2/50, Batch 20/61, Loss: 0.1575\n",
            "Epoch 2/50, Batch 30/61, Loss: 0.2121\n",
            "Epoch 2/50, Batch 40/61, Loss: 0.0850\n",
            "Epoch 2/50, Batch 50/61, Loss: 0.0402\n",
            "Epoch 2/50, Batch 60/61, Loss: 0.0657\n",
            "Epoch 2/50\n",
            "Train Loss: 0.1755, Train Acc: 93.15%\n",
            "Val Loss: 0.2302, Val Acc: 90.28%\n",
            "Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 1/10\n",
            "Epoch 3/50, Batch 0/61, Loss: 0.0412\n",
            "Epoch 3/50, Batch 10/61, Loss: 0.0583\n",
            "Epoch 3/50, Batch 20/61, Loss: 0.0146\n",
            "Epoch 3/50, Batch 30/61, Loss: 0.1609\n",
            "Epoch 3/50, Batch 40/61, Loss: 0.1695\n",
            "Epoch 3/50, Batch 50/61, Loss: 0.0544\n",
            "Epoch 3/50, Batch 60/61, Loss: 0.0070\n",
            "Epoch 3/50\n",
            "Train Loss: 0.1095, Train Acc: 96.31%\n",
            "Val Loss: 0.2351, Val Acc: 92.89%\n",
            "Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "ğŸ’¾ New best model saved! Validation Accuracy: 92.89%\n",
            "Epoch 4/50, Batch 0/61, Loss: 0.1164\n",
            "Epoch 4/50, Batch 10/61, Loss: 0.2138\n",
            "Epoch 4/50, Batch 20/61, Loss: 0.2135\n",
            "Epoch 4/50, Batch 30/61, Loss: 0.1057\n",
            "Epoch 4/50, Batch 40/61, Loss: 0.0304\n",
            "Epoch 4/50, Batch 50/61, Loss: 0.1333\n",
            "Epoch 4/50, Batch 60/61, Loss: 0.0080\n",
            "Epoch 4/50\n",
            "Train Loss: 0.0749, Train Acc: 97.25%\n",
            "Val Loss: 0.2558, Val Acc: 91.23%\n",
            "Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 1/10\n",
            "Epoch 5/50, Batch 0/61, Loss: 0.0384\n",
            "Epoch 5/50, Batch 10/61, Loss: 0.0287\n",
            "Epoch 5/50, Batch 20/61, Loss: 0.0108\n",
            "Epoch 5/50, Batch 30/61, Loss: 0.0878\n",
            "Epoch 5/50, Batch 40/61, Loss: 0.0135\n",
            "Epoch 5/50, Batch 50/61, Loss: 0.0991\n",
            "Epoch 5/50, Batch 60/61, Loss: 0.0061\n",
            "Epoch 5/50\n",
            "Train Loss: 0.0777, Train Acc: 96.99%\n",
            "Val Loss: 0.2558, Val Acc: 92.89%\n",
            "Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 2/10\n",
            "Epoch 6/50, Batch 0/61, Loss: 0.0507\n",
            "Epoch 6/50, Batch 10/61, Loss: 0.0639\n",
            "Epoch 6/50, Batch 20/61, Loss: 0.0396\n",
            "Epoch 6/50, Batch 30/61, Loss: 0.0627\n",
            "Epoch 6/50, Batch 40/61, Loss: 0.1003\n",
            "Epoch 6/50, Batch 50/61, Loss: 0.0239\n",
            "Epoch 6/50, Batch 60/61, Loss: 0.4934\n",
            "Epoch 6/50\n",
            "Train Loss: 0.0609, Train Acc: 98.23%\n",
            "Val Loss: 0.4837, Val Acc: 88.39%\n",
            "Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 3/10\n",
            "Epoch 7/50, Batch 0/61, Loss: 0.0745\n",
            "Epoch 7/50, Batch 10/61, Loss: 0.1761\n",
            "Epoch 7/50, Batch 20/61, Loss: 0.0226\n",
            "Epoch 7/50, Batch 30/61, Loss: 0.1858\n",
            "Epoch 7/50, Batch 40/61, Loss: 0.0211\n",
            "Epoch 7/50, Batch 50/61, Loss: 0.0183\n",
            "Epoch 7/50, Batch 60/61, Loss: 0.0009\n",
            "Epoch 7/50\n",
            "Train Loss: 0.1236, Train Acc: 95.59%\n",
            "Val Loss: 0.2325, Val Acc: 91.23%\n",
            "Learning Rate: 0.000500\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 4/10\n",
            "Epoch 8/50, Batch 0/61, Loss: 0.0775\n",
            "Epoch 8/50, Batch 10/61, Loss: 0.0543\n",
            "Epoch 8/50, Batch 20/61, Loss: 0.0252\n",
            "Epoch 8/50, Batch 30/61, Loss: 0.0263\n",
            "Epoch 8/50, Batch 40/61, Loss: 0.0181\n",
            "Epoch 8/50, Batch 50/61, Loss: 0.0342\n",
            "Epoch 8/50, Batch 60/61, Loss: 0.0285\n",
            "Epoch 8/50\n",
            "Train Loss: 0.0528, Train Acc: 97.98%\n",
            "Val Loss: 0.2678, Val Acc: 90.76%\n",
            "Learning Rate: 0.000500\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 5/10\n",
            "Epoch 9/50, Batch 0/61, Loss: 0.0132\n",
            "Epoch 9/50, Batch 10/61, Loss: 0.0133\n",
            "Epoch 9/50, Batch 20/61, Loss: 0.0527\n",
            "Epoch 9/50, Batch 30/61, Loss: 0.0505\n",
            "Epoch 9/50, Batch 40/61, Loss: 0.0139\n",
            "Epoch 9/50, Batch 50/61, Loss: 0.0670\n",
            "Epoch 9/50, Batch 60/61, Loss: 0.0011\n",
            "Epoch 9/50\n",
            "Train Loss: 0.0391, Train Acc: 98.60%\n",
            "Val Loss: 0.3433, Val Acc: 92.42%\n",
            "Learning Rate: 0.000500\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 6/10\n",
            "Epoch 10/50, Batch 0/61, Loss: 0.0116\n",
            "Epoch 10/50, Batch 10/61, Loss: 0.0070\n",
            "Epoch 10/50, Batch 20/61, Loss: 0.0100\n",
            "Epoch 10/50, Batch 30/61, Loss: 0.0617\n",
            "Epoch 10/50, Batch 40/61, Loss: 0.0272\n",
            "Epoch 10/50, Batch 50/61, Loss: 0.2643\n",
            "Epoch 10/50, Batch 60/61, Loss: 0.0010\n",
            "Epoch 10/50\n",
            "Train Loss: 0.0217, Train Acc: 99.48%\n",
            "Val Loss: 0.3253, Val Acc: 92.42%\n",
            "Learning Rate: 0.000500\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 7/10\n",
            "Epoch 11/50, Batch 0/61, Loss: 0.0109\n",
            "Epoch 11/50, Batch 10/61, Loss: 0.0040\n",
            "Epoch 11/50, Batch 20/61, Loss: 0.0199\n",
            "Epoch 11/50, Batch 30/61, Loss: 0.0054\n",
            "Epoch 11/50, Batch 40/61, Loss: 0.0029\n",
            "Epoch 11/50, Batch 50/61, Loss: 0.0013\n",
            "Epoch 11/50, Batch 60/61, Loss: 0.0001\n",
            "Epoch 11/50\n",
            "Train Loss: 0.0182, Train Acc: 99.53%\n",
            "Val Loss: 0.3486, Val Acc: 91.71%\n",
            "Learning Rate: 0.000500\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 8/10\n",
            "Epoch 12/50, Batch 0/61, Loss: 0.0156\n",
            "Epoch 12/50, Batch 10/61, Loss: 0.0101\n",
            "Epoch 12/50, Batch 20/61, Loss: 0.1278\n",
            "Epoch 12/50, Batch 30/61, Loss: 0.0058\n",
            "Epoch 12/50, Batch 40/61, Loss: 0.0007\n",
            "Epoch 12/50, Batch 50/61, Loss: 0.0009\n",
            "Epoch 12/50, Batch 60/61, Loss: 0.0001\n",
            "Epoch 12/50\n",
            "Train Loss: 0.0167, Train Acc: 99.38%\n",
            "Val Loss: 0.3612, Val Acc: 91.71%\n",
            "Learning Rate: 0.000500\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 9/10\n",
            "Epoch 13/50, Batch 0/61, Loss: 0.0127\n",
            "Epoch 13/50, Batch 10/61, Loss: 0.0731\n",
            "Epoch 13/50, Batch 20/61, Loss: 0.0067\n",
            "Epoch 13/50, Batch 30/61, Loss: 0.0004\n",
            "Epoch 13/50, Batch 40/61, Loss: 0.0022\n",
            "Epoch 13/50, Batch 50/61, Loss: 0.0020\n",
            "Epoch 13/50, Batch 60/61, Loss: 0.0191\n",
            "Epoch 13/50\n",
            "Train Loss: 0.0179, Train Acc: 99.33%\n",
            "Val Loss: 0.3323, Val Acc: 92.42%\n",
            "Learning Rate: 0.000250\n",
            "------------------------------------------------------------\n",
            "â³ Patience: 10/10\n",
            "ğŸ›‘ Early stopping triggered at epoch 13\n",
            "Best validation accuracy: 92.89%\n",
            "âœ… Best model loaded with validation accuracy: 92.89%\n",
            "\n",
            "ğŸ FINAL EVALUATION ON VALIDATION SET\n",
            "ğŸ¯==================================================\n",
            "ğŸ“Š FINAL EVALUATION METRICS\n",
            "===================================================\n",
            "Accuracy  : 0.9289\n",
            "Precision : 0.9314\n",
            "Recall    : 0.9289\n",
            "F1 Score  : 0.9299\n",
            "===================================================\n",
            "\n",
            "ğŸ“‹ Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.79      0.85      0.82        79\n",
            "        male       0.96      0.95      0.96       343\n",
            "\n",
            "    accuracy                           0.93       422\n",
            "   macro avg       0.88      0.90      0.89       422\n",
            "weighted avg       0.93      0.93      0.93       422\n",
            "\n",
            "\n",
            "ğŸ¯ Class-wise Accuracy:\n",
            "female: 84.81% (67/79)\n",
            "male: 94.75% (325/343)\n",
            "\n",
            "ğŸ’¾ Final model saved as 'final_gender_classification_model.pth'\n",
            "âœ… Training completed successfully!\n",
            "\n",
            "ğŸ‰ ALL DONE! Your model should now perform significantly better than 90.28%!\n"
          ]
        }
      ],
      "source": [
        "# Complete EfficientNet Gender Classification Code for Hackathon\n",
        "# Install required packages\n",
        "!pip install -q torch torchvision scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "train_dir = '/content/drive/MyDrive/FACECOM/Task_A/train'\n",
        "val_dir = '/content/drive/MyDrive/FACECOM/Task_A/val'\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Advanced data augmentation for training\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Simple transform for validation (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = ImageFolder(train_dir, transform=train_transform)\n",
        "val_dataset = ImageFolder(val_dir, transform=val_transform)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "\n",
        "# Handle class imbalance with weighted sampler\n",
        "class_counts = Counter(train_dataset.targets)\n",
        "print(f\"Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "# Calculate weights to balance classes\n",
        "total_samples = len(train_dataset)\n",
        "class_weights = {}\n",
        "for cls, count in class_counts.items():\n",
        "    class_weights[cls] = total_samples / (len(class_counts) * count)\n",
        "\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# Create sample weights for each training sample\n",
        "sample_weights = [class_weights[target] for target in train_dataset.targets]\n",
        "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Create EfficientNet-B0 model\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "print(f\"Original classifier: {model.classifier}\")\n",
        "\n",
        "# Replace the classifier for binary classification\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Modified classifier: {model.classifier}\")\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "# Training function with early stopping\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, patience=10):\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Print progress every 10 batches\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_acc = accuracy_score(all_labels, all_preds) * 100\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "            }, 'best_gender_model.pth')\n",
        "            print(f\"ğŸ’¾ New best model saved! Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"â³ Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"ğŸ›‘ Early stopping triggered at epoch {epoch+1}\")\n",
        "                print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load('best_gender_model.pth')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"âœ… Best model loaded with validation accuracy: {checkpoint['best_val_acc']:.2f}%\")\n",
        "\n",
        "    return model, train_losses, val_losses, val_accuracies\n",
        "\n",
        "# Detailed evaluation function\n",
        "def evaluate_model(model, val_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Get probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    print(\"ğŸ¯\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š FINAL EVALUATION METRICS\")\n",
        "    print(\"=\"*51)\n",
        "    print(f\"Accuracy  : {accuracy:.4f}\")\n",
        "    print(f\"Precision : {precision:.4f}\")\n",
        "    print(f\"Recall    : {recall:.4f}\")\n",
        "    print(f\"F1 Score  : {f1:.4f}\")\n",
        "    print(\"=\"*51)\n",
        "    print(\"\\nğŸ“‹ Detailed Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['female', 'male']))\n",
        "\n",
        "    # Class-wise accuracy\n",
        "    class_correct = [0, 0]\n",
        "    class_total = [0, 0]\n",
        "    for i in range(len(all_labels)):\n",
        "        label = all_labels[i]\n",
        "        class_total[label] += 1\n",
        "        if all_preds[i] == label:\n",
        "            class_correct[label] += 1\n",
        "\n",
        "    print(\"\\nğŸ¯ Class-wise Accuracy:\")\n",
        "    for i, class_name in enumerate(['female', 'male']):\n",
        "        if class_total[i] > 0:\n",
        "            acc = 100 * class_correct[i] / class_total[i]\n",
        "            print(f\"{class_name}: {acc:.2f}% ({class_correct[i]}/{class_total[i]})\")\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Start training\n",
        "print(\"ğŸš€ Starting EfficientNet-B0 Training for Gender Classification\")\n",
        "print(f\"ğŸ“Š Dataset Info:\")\n",
        "print(f\"   Training samples: {len(train_dataset)}\")\n",
        "print(f\"   Validation samples: {len(val_dataset)}\")\n",
        "print(f\"   Classes: {train_dataset.classes}\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train the model\n",
        "trained_model, train_losses, val_losses, val_accuracies = train_model(\n",
        "    model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "    num_epochs=50, patience=10\n",
        ")\n",
        "\n",
        "# Final evaluation\n",
        "print(\"\\nğŸ FINAL EVALUATION ON VALIDATION SET\")\n",
        "accuracy, precision, recall, f1 = evaluate_model(trained_model, val_loader, device)\n",
        "\n",
        "# Save final model for submission\n",
        "torch.save({\n",
        "    'model_state_dict': trained_model.state_dict(),\n",
        "    'model_architecture': 'efficientnet_b0',\n",
        "    'num_classes': 2,\n",
        "    'class_names': train_dataset.classes,\n",
        "    'accuracy': accuracy,\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1_score': f1\n",
        "}, 'final_gender_classification_model.pth')\n",
        "\n",
        "print(\"\\nğŸ’¾ Final model saved as 'final_gender_classification_model.pth'\")\n",
        "print(\"âœ… Training completed successfully!\")\n",
        "\n",
        "print(\"\\nğŸ‰ ALL DONE! Your model should now perform significantly better than 90.28%!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNs+U6/wfwguWFWTBwULmhO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}